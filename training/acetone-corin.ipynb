{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "import torch\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "import e3nn\n",
    "import e3nn.point.data_helpers as dh \n",
    "from training_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-f7b18b5e02e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# make sure CUDA is available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/e3nn/lib/python3.8/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mcurrent_device\u001b[0;34m()\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcurrent_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;34mr\"\"\"Returns the index of a currently selected device.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_getDevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/e3nn/lib/python3.8/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m             raise RuntimeError(\n\u001b[1;32m    148\u001b[0m                 \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             raise AssertionError(\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/e3nn/lib/python3.8/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_check_driver\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_cuda_isDriverSufficient'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_isDriverSufficient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_getDriverVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# make sure CUDA is available\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "device = \"cuda\"\n",
    "#torch.rand(10).to(device)\n",
    "#torch.rand(10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 8 6 1 1 1 6 1 1 1]\n",
      "[ -20.1123 -337.7406  159.0655   29.4662   29.4664   29.7726  159.0733\n",
      "   29.4659   29.4658   29.773 ]\n"
     ]
    }
   ],
   "source": [
    "# read stationary data\n",
    "with h5py.File(\"acetone/acetone-b3lyp_d3bj-631gd-gas-equilibrium_geometry.hdf5\", \"r\") as h5:\n",
    "    atomic_numbers = np.array(h5.get(\"atomic_numbers\"))\n",
    "    isotropic_shieldings = np.array(h5.get(\"isotropic_shieldings\"))\n",
    "print(atomic_numbers)\n",
    "print(isotropic_shieldings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 10, 4)\n"
     ]
    }
   ],
   "source": [
    "# read perturbed data\n",
    "with h5py.File(\"acetone/acetone-b3lyp_d3bj-631gd-gas-NMR-pcSseg_1.hdf5\", \"r\") as h5:\n",
    "    geoms_and_shieldings = np.array(h5.get(\"data\"))\n",
    "print(np.shape(geoms_and_shieldings))\n",
    "geometries = geoms_and_shieldings[:,:,:3]\n",
    "shieldings = geoms_and_shieldings[:,:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data on a per-site basis\n",
    "# this is extreme, but let's see how well we can do with this\n",
    "means = np.mean(shieldings, axis=0)\n",
    "stdevs = np.std(shieldings, axis=0)\n",
    "shieldings = (shieldings-means)/stdevs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will undo scaling so we can recover the absolute shieldings from predictions\n",
    "def undo_scaling(v):\n",
    "    return v * stdevs + means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate one-hot features\n",
    "one_hots = pd.get_dummies(atomic_numbers)\n",
    "one_hots = one_hots.to_numpy()\n",
    "\n",
    "# get some shapes\n",
    "n_examples = geometries.shape[0]\n",
    "n_atoms_per_example = geometries.shape[1]\n",
    "n_one_hots = one_hots.shape[1]\n",
    "\n",
    "# features are just one-hots, so repeat the same one-hots for every training example\n",
    "desired_shape = (n_examples, n_atoms_per_example, n_one_hots)\n",
    "features = np.zeros(desired_shape)\n",
    "features[:] = one_hots              # broadcast one_hots into every row along axis 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features are one-hots for every atom,\n",
    "# so this is (number of one_hots, rank zero tensor, even parity)\n",
    "Rs_in = [(n_one_hots,0,1)]\n",
    "\n",
    "# we are outputing one scalar for every atom\n",
    "# so this is (one, rank zero tensor, even parity)\n",
    "Rs_out = [(1,0,1)]\n",
    "\n",
    "# maximum extent of radial basis functions in Angstroms\n",
    "max_radius = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataNeighborsSelection(dh.DataNeighbors):\n",
    "    def __init__(self, x, Rs_in, pos, r_max, atomic_numbers, relevant_atomic_numbers=None, self_interaction=True, **kwargs):\n",
    "        if atomic_numbers is None:\n",
    "            raise ValueError(\"atomic_numbers must be defined\")\n",
    "        if relevant_atomic_numbers is None:\n",
    "            relevant_atomic_numbers = [1, 6]  \n",
    "        relevant_atom_index = [i for i, atom_number in enumerate(atomic_numbers)\n",
    "                               if atom_number in relevant_atomic_numbers]\n",
    "        relevant_atom_index = torch.LongTensor(relevant_atom_index).unsqueeze(0)  # [1, N]\n",
    "        \n",
    "        super().__init__(x, Rs_in, pos, r_max,\n",
    "                         self_interaction=self_interaction,\n",
    "                         atomic_numbers=atomic_numbers, \n",
    "                         relevant_atom_index=relevant_atom_index,\n",
    "                         **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a subset of the data to a .torch file\n",
    "# precompute the relevant neighbors\n",
    "relevant_atomic_numbers = [1, 6]\n",
    "\n",
    "def save_dataset(geometry_subset, shielding_subset, feature_subset, output_filename):\n",
    "    dataset = []\n",
    "    n_to_save = len(geometry_subset)\n",
    "    print(f\"Preprocessing for {output_filename}:\")\n",
    "    for i,(g,s,f) in enumerate(zip(geometry_subset, shielding_subset, feature_subset)):\n",
    "        g,s,f = torch.tensor(g, dtype=torch.float64), torch.tensor(s, dtype=torch.float64), torch.tensor(f, dtype=torch.float64)\n",
    "        data = DataNeighborsSelection(x=f, Rs_in=Rs_in, pos=g, r_max=max_radius,\n",
    "                                      atomic_numbers=atomic_numbers, relevant_atomic_numbers=relevant_atomic_numbers,\n",
    "                                      y=s, Rs_out=Rs_out)\n",
    "        dataset.append(data)\n",
    "        if (i+1) % 100 == 0 or i == n_to_save - 1:\n",
    "            print(f\"{i+1:10d} of {n_to_save:10d}...\", end=\"\\r\", flush=True)\n",
    "    print(\"\\nWriting to disk...\", end=\"\", flush=True)\n",
    "    torch.save(dataset, output_filename)\n",
    "    print(f\"done.  Saved {len(dataset)} records.\")\n",
    "    \n",
    "# splits the dataset randomly into training and test sets\n",
    "# train_size = # number of training examples\n",
    "# test_size = # number of test examples\n",
    "# prefix = filenames will start with this string\n",
    "# random_state = for reproducible splits\n",
    "def split_and_save(train_size, test_size, prefix, random_state):\n",
    "    assert train_size + test_size <= n_examples\n",
    "    # [ training_geometries, training_shieldings, training_features,\n",
    "    #   testing_geometries, testing_shieldings, testing_features     ]\n",
    "    splitting = train_test_split(geometries, shieldings, features,\n",
    "                                 test_size = test_size, train_size = train_size,\n",
    "                                 random_state = random_state, shuffle = True)\n",
    "    save_dataset(*splitting[::2], f\"{prefix}-train.torch\")\n",
    "    save_dataset(*splitting[1::2], f\"{prefix}-test.torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing for acetone-split1-train.torch:\n",
      "       500 of        500...\n",
      "Writing to disk...done.  Saved 500 records.\n",
      "Preprocessing for acetone-split1-test.torch:\n",
      "      5000 of       5000...\n",
      "Writing to disk...done.  Saved 5000 records.\n"
     ]
    }
   ],
   "source": [
    "train_size = 500\n",
    "test_size = 5000\n",
    "split_and_save(train_size, test_size, \"acetone-split1\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = torch.load(\"acetone-split1-train.torch\")\n",
    "dataset2 = torch.load(\"acetone-split1-test.torch\")\n",
    "batch_size = 5\n",
    "train_dataloader = tg.data.DataListLoader(dataset1, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = tg.data.DataListLoader(dataset2, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE loss function for this system\n",
    "# only incorporate elements of interest\n",
    "# relevant_atomic_numbers = {1,6}\n",
    "# def loss_function_generator(atomic_numbers):\n",
    "#     indices = []\n",
    "#     for i,atomic_number in enumerate(atomic_numbers):\n",
    "#         if atomic_number in relevant_atomic_numbers:\n",
    "#             indices.append(i)\n",
    "#     indices = indices * batch_size\n",
    "            \n",
    "#     def loss_function(x,y):\n",
    "#         error = (x-y)[indices]**2.0\n",
    "#         return error.mean()\n",
    "    \n",
    "#     return loss_function\n",
    "# loss_function = loss_function_generator(atomic_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the neural network architecture\n",
    "model_kwargs = {\n",
    "    'network': 'GatedConvParityNetwork', \n",
    "    'conv': 'Convolution',\n",
    "    'Rs_in': Rs_in,            # shape of inputs\n",
    "    'Rs_out': Rs_out,          # shape of outputs\n",
    "    'mul': 5,                 # how many copies of each tensor at each layer\n",
    "    'lmax': 2,                 # maximum angular momentum\n",
    "    'layers': 3,               # number of layers\n",
    "    'max_radius': max_radius,  # radial kernel will extend out this far\n",
    "    'number_of_basis': 10,     # number of Gaussians in radial kernel\n",
    "}\n",
    "\n",
    "model_kwargs = {\n",
    "    'network': 'GatedConvParityNetwork', \n",
    "    'conv': 'Convolution',\n",
    "    'Rs_in': Rs_in,            # shape of inputs\n",
    "    'Rs_out': Rs_out,          # shape of outputs\n",
    "    'mul': 5,                 # how many copies of each tensor at each layer\n",
    "    'lmax': 2,                 # maximum angular momentum\n",
    "    'layers': 3,               # number of layers\n",
    "    'max_radius': max_radius,  # radial kernel will extend out this far\n",
    "    'number_of_basis': 10,     # number of Gaussians in radial kernel\n",
    "}\n",
    "\n",
    "model = model_from_kwargs(model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test the model as it now\n",
    "# # n_norm is average number of convolution neighbors per atom\n",
    "# model.to(device)\n",
    "# start_time = time.time()\n",
    "# losses = evaluate(model, test_dataloader, [loss_function], device, n_norm=5)\n",
    "# end_time = time.time()\n",
    "# elapsed_time = end_time - start_time\n",
    "# print(f\"Loss is {losses[0].data:.4f}.  Took {elapsed_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "learning_rate = 3e-3\n",
    "opt = torch.optim.Adam(model.parameters(), learning_rate)\n",
    "max_iter = 100       \n",
    "n_norm = 5           # n_norm is average number of convolution neighbors per atom\n",
    "n_batches = int(train_size / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration     1    batch     3 /   100  loss =  0.5958    elapsed =    0.57 s\n",
      "Iteration     2    batch     3 /   100  loss =  0.9471    elapsed =    0.50 s\n",
      "Iteration     3    batch     3 /   100  loss =  0.6032    elapsed =    0.53 s\n",
      "Iteration     4    batch     3 /   100  loss =  0.4546    elapsed =    0.51 s\n",
      "Iteration     5    batch     3 /   100  loss =  0.4161    elapsed =    0.51 s\n",
      "Iteration     6    batch     3 /   100  loss =  0.4712    elapsed =    0.50 s\n",
      "Iteration     7    batch     3 /   100  loss =  0.8175    elapsed =    0.51 s\n",
      "Iteration     8    batch     3 /   100  loss =  0.6472    elapsed =    0.51 s\n",
      "Iteration     9    batch     3 /   100  loss =  0.5778    elapsed =    0.51 s\n",
      "Iteration    10    batch     3 /   100  loss =  0.5404    elapsed =    0.50 s\n",
      "Iteration    11    batch     3 /   100  loss =  0.6549    elapsed =    0.51 s\n",
      "Iteration    12    batch     3 /   100  loss =  0.5936    elapsed =    0.51 s\n",
      "Iteration    13    batch     3 /   100  loss =  0.9196    elapsed =    0.50 s\n",
      "Iteration    14    batch     3 /   100  loss =  0.7468    elapsed =    0.51 s\n",
      "Iteration    15    batch     3 /   100  loss =  0.5790    elapsed =    0.52 s\n",
      "Iteration    16    batch     3 /   100  loss =  0.6354    elapsed =    0.51 s\n",
      "Iteration    17    batch     3 /   100  loss =  0.5352    elapsed =    0.53 s\n",
      "Iteration    18    batch     3 /   100  loss =  0.7424    elapsed =    0.53 s\n",
      "Iteration    19    batch     3 /   100  loss =  0.6741    elapsed =    0.51 s\n",
      "Iteration    20    batch     3 /   100  loss =  0.6421    elapsed =    0.52 s\n",
      "Iteration    21    batch     3 /   100  loss =  0.6827    elapsed =    0.52 s\n",
      "Iteration    22    batch     3 /   100  loss =  0.5497    elapsed =    0.53 s\n",
      "Iteration    23    batch     3 /   100  loss =  0.6684    elapsed =    0.53 s\n",
      "Iteration    24    batch     3 /   100  loss =  0.6301    elapsed =    0.54 s\n",
      "Iteration    25    batch     3 /   100  loss =  0.6810    elapsed =    0.53 s\n",
      "Iteration    26    batch     3 /   100  loss =  0.7246    elapsed =    0.54 s\n",
      "Iteration    27    batch     3 /   100  loss =  0.7351    elapsed =    0.57 s\n",
      "Iteration    28    batch     3 /   100  loss =  0.7017    elapsed =    0.54 s\n",
      "Iteration    29    batch     3 /   100  loss =  0.6577    elapsed =    0.55 s\n",
      "Iteration    30    batch     3 /   100  loss =  0.6786    elapsed =    0.55 s\n",
      "Iteration    31    batch     3 /   100  loss =  0.6505    elapsed =    0.54 s\n",
      "Iteration    32    batch     3 /   100  loss =  0.4955    elapsed =    0.54 s\n",
      "Iteration    33    batch     3 /   100  loss =  0.7170    elapsed =    0.52 s\n",
      "Iteration    34    batch     3 /   100  loss =  0.6876    elapsed =    0.53 s\n",
      "Iteration    35    batch     3 /   100  loss =  0.5910    elapsed =    0.54 s\n",
      "Iteration    36    batch     3 /   100  loss =  0.6216    elapsed =    0.53 s\n",
      "Iteration    37    batch     1 /   100\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-c1005e50b281>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss_cum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/e3nn/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/e3nn/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "for i in range(max_iter):\n",
    "    start_time = time.time()\n",
    "    loss_cum = torch.tensor(0.)\n",
    "    data_good = None\n",
    "    for j,data in enumerate(train_dataloader):\n",
    "        if j == 2:\n",
    "            break\n",
    "        print(f\"Iteration {i+1:5d}    batch {j+1:5d} / {n_batches:5d}\", end=\"\\r\", flush=True)\n",
    "        data = tg.data.Batch.from_data_list(data)\n",
    "        data.to(device)\n",
    "        output = model(data.x, data.edge_index, data.edge_attr, n_norm=n_norm)\n",
    "\n",
    "        loss = ((output[data.relevant_atom_index] - data.y[data.relevant_atom_index]) ** 2).mean()\n",
    "#         loss = loss_function(output, data.y)\n",
    "        loss_cum += loss\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    end_time = time.time()\n",
    "    elasped_time = end_time - start_time\n",
    "    print(f\"Iteration {i+1:5d}    batch {j+1:5d} / {n_batches:5d}  loss = {loss_cum.data / (j+1):7.4f}    elapsed = {elasped_time:7.2f} s\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test the model as it now\n",
    "# # n_norm is average number of convolution neighbors per atom\n",
    "# losses = evaluate(model, test_dataloader, [loss_function], device, n_norm=5)\n",
    "# print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
