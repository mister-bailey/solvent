{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pprint import pprint\n",
    "from IPython.display import display\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "import e3nn\n",
    "import e3nn.point.data_helpers as dh \n",
    "from training_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "Tesla V100-PCIE-32GB\n",
      "True\n",
      "10.1\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# make sure CUDA is available\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "#print(torch.cuda.memory_summary())\n",
    "device = \"cuda\"\n",
    "#torch.rand(10).to(device)\n",
    "#torch.rand(10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_elements = { \"H\", \"C\" }                            # only train on the shieldings at these elements\n",
    "elementwide_scaling_factors = {\"C\":5.0, \"H\":1.5, \"O\":50.0}  # divide absolute shieldings by these numbers\n",
    "n_elements = len(elementwide_scaling_factors)\n",
    "all_elements = list(elementwide_scaling_factors.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# represents the training data for one molecule\n",
    "# all of these items are stored on the cpu\n",
    "class Molecule():\n",
    "    def __init__(self, name,               # name of molecule\n",
    "                 atomic_symbols,           # vector of strings of length n_atoms\n",
    "                 symmetrical_atoms,        # list of lists of 0-indexed atom numbers\n",
    "                 stationary_shieldings,    # vector of floats of length n_atoms\n",
    "                 geometries,               # (n_examples, n_atoms, 3)\n",
    "                 perturbed_shieldings):    # (n_examples, n_atoms, 1) \n",
    "        self.name = name                                       \n",
    "        self.stationary_shieldings = stationary_shieldings\n",
    "        self.geometries = geometries                                               \n",
    "        self.atomic_symbols = atomic_symbols\n",
    "        self.n_atoms = len(atomic_symbols)\n",
    "  \n",
    "        # rescale shieldings for training\n",
    "        perturbed_shieldings = perturbed_shieldings - stationary_shieldings\n",
    "        scaling_factors = [ elementwide_scaling_factors[symbol] for symbol in atomic_symbols ]\n",
    "        scaling_factors = np.array(scaling_factors)\n",
    "        self.scaling_factors = scaling_factors\n",
    "        perturbed_shieldings = perturbed_shieldings / scaling_factors\n",
    "        self.perturbed_shieldings = perturbed_shieldings\n",
    "        \n",
    "        # compute features\n",
    "        # one-hots for one example (since they're all the same): n_atoms, n_elements\n",
    "        features = []\n",
    "        for symbol,shielding in zip(atomic_symbols,stationary_shieldings):\n",
    "            inner_list = [ 1. if symbol == i else 0. for i in all_elements ]\n",
    "            #inner_list.append(shielding)\n",
    "            features.append(inner_list)\n",
    "        self.features = np.array(features)\n",
    "    \n",
    "        # compute per-atom weights for the loss function\n",
    "        weights = [ 1.0 if symbol in relevant_elements else 0.0 for symbol in atomic_symbols ]\n",
    "        weights = np.array(weights)\n",
    "        for l in symmetrical_atoms:\n",
    "            weight = 1.0/len(l)\n",
    "            for i in l:\n",
    "                weights[i] = weight\n",
    "        self.weights = weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acetaldehyde\n",
      "acetone\n",
      "butanone\n",
      "dimethyl_ether\n",
      "ethane\n"
     ]
    }
   ],
   "source": [
    "molecules_dict = {}  # name -> Molecule\n",
    "\n",
    "# read the training data\n",
    "# iterate through the hdf5s (one per molecule)\n",
    "for hdf5_filename in sorted(glob(\"*.hdf5\")):\n",
    "    with h5py.File(hdf5_filename, \"r\") as h5:\n",
    "        name = h5.attrs.get(\"name\")\n",
    "        print(name)\n",
    "        geometries_and_shieldings = np.array(h5.get(\"data\"))\n",
    "        geometries = geometries_and_shieldings[:,:,:3]\n",
    "        perturbed_shieldings = geometries_and_shieldings[:,:,3]\n",
    "        stationary_shieldings = np.array(h5.attrs.get(\"stationary\"))\n",
    "        atomic_symbols = list(h5.get(\"atomic_symbols\"))\n",
    "        atomic_symbols = [ symbol.decode(\"utf-8\") for symbol in atomic_symbols ]\n",
    "        n_atoms = len(atomic_symbols)\n",
    "\n",
    "        # these are the 1-indexed atom numbers that are symmetrical\n",
    "        group = h5.get(\"symmetrical_atoms\")\n",
    "        symmetrical_atoms = []  # 0-indexed\n",
    "        for v in group.values():\n",
    "            v = [ i-1 for i in v ]\n",
    "            symmetrical_atoms.append(v)\n",
    "\n",
    "        # store the results\n",
    "        molecule = Molecule(name, atomic_symbols, symmetrical_atoms,\n",
    "                            stationary_shieldings, geometries, perturbed_shieldings)\n",
    "        molecules_dict[name] = molecule\n",
    "        \n",
    "molecules = np.array(list(molecules_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features are one-hots for every atom,\n",
    "# so this is (number of one_hots, rank zero tensor, even parity)\n",
    "Rs_in = [(n_elements,0,1)]\n",
    "\n",
    "# we are outputing one scalar for every atom\n",
    "# so this is (one, rank zero tensor, even parity)\n",
    "Rs_out = [(1,0,1)]\n",
    "\n",
    "# maximum extent of radial basis functions in Angstroms\n",
    "max_radius = 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the neighbor information for the train and test sets\n",
    "def create_torch_datasets(molecules, train_size, test_size, random_state):\n",
    "    training_dataset = []\n",
    "    testing_dataset = []\n",
    "    for molecule in molecules:\n",
    "        print(f\"Preprocessing {molecule.name} data:\")\n",
    "        train_geometries, test_geometries, train_shieldings, test_shieldings = train_test_split(\n",
    "                                                              molecule.geometries,\n",
    "                                                              molecule.perturbed_shieldings,\n",
    "                                                              train_size=train_size, test_size=test_size,\n",
    "                                                              random_state=random_state)\n",
    "        train_test = [(train_geometries,train_shieldings,training_dataset),\n",
    "                      (test_geometries,test_shieldings,testing_dataset)]\n",
    "        features = torch.tensor(molecule.features, dtype=torch.float64)\n",
    "        weights = torch.tensor(molecule.weights, dtype=torch.float64) \n",
    "        i = 0\n",
    "        n_to_save = len(train_geometries) + len(test_geometries)\n",
    "        for geometries, shieldings, target in train_test:\n",
    "            for g,s in zip(geometries,shieldings):\n",
    "                g = torch.tensor(g, dtype=torch.float64)\n",
    "                s = torch.tensor(s, dtype=torch.float64).unsqueeze(-1)  # [1,N]\n",
    "                data = dh.DataNeighbors(x=features, Rs_in=Rs_in, pos=g, r_max=max_radius,\n",
    "                                        self_interaction=True, name=molecule.name,\n",
    "                                        weights=weights, y=s, Rs_out = Rs_out)\n",
    "                target.append(data)\n",
    "                i += 1\n",
    "                if (i+1) % 100 == 0:\n",
    "                    print(f\"{i+1:10d} of {n_to_save:10d}...\", end=\"\\r\", flush=True)\n",
    "                if i == n_to_save - 1:\n",
    "                    print(f\"{i+1:10d} of {n_to_save:10d}               done!\")\n",
    "    return training_dataset, testing_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing acetaldehyde data:\n",
      "      2000 of       2000               done!\n",
      "Preprocessing acetone data:\n",
      "      2000 of       2000               done!\n",
      "Preprocessing butanone data:\n",
      "      2000 of       2000               done!\n",
      "Preprocessing dimethyl_ether data:\n",
      "      2000 of       2000               done!\n",
      "Preprocessing ethane data:\n",
      "      2000 of       2000               done!\n"
     ]
    }
   ],
   "source": [
    "training_dataset, test_dataset = create_torch_datasets(molecules,\n",
    "                                                       train_size = 1000,\n",
    "                                                       test_size = 1000,\n",
    "                                                       random_state = 3)\n",
    "# training_dataset, test_dataset = create_torch_datasets(training_molecules,\n",
    "#                                                        train_size = 20000,\n",
    "#                                                        test_size = 5000,\n",
    "#                                                        random_state = 1)\n",
    "# _, final_testing_dataset = create_torch_datasets(testing_molecules,\n",
    "#                                                  train_size = 1,\n",
    "#                                                  test_size = 5000,\n",
    "#                                                  random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean-squared loss\n",
    "def loss_function(output, data):\n",
    "    predictions = output\n",
    "    observations = data.y\n",
    "    weights = data.weights\n",
    "    normalization = weights.sum()\n",
    "    residuals = (predictions-observations)\n",
    "    loss = residuals.square() * weights\n",
    "    loss = loss.sum() / normalization\n",
    "    loss = loss.pow(0.5)\n",
    "    return loss, residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the neural network architecture\n",
    "model_kwargs = {\n",
    "    'network': 'GatedConvParityNetwork', \n",
    "    'conv': 'Convolution',\n",
    "    'Rs_in': Rs_in,            # shape of inputs\n",
    "    'Rs_out': Rs_out,          # shape of outputs\n",
    "    'mul': 3,                 # how many copies of each tensor at each layer\n",
    "    'lmax': 1,                 # maximum angular momentum\n",
    "    'layers': 7,               # number of layers\n",
    "    'max_radius': max_radius,  # radial kernel will extend out this far\n",
    "    'number_of_basis': 10,     # number of Gaussians in radial kernel\n",
    "}\n",
    "model = model_from_kwargs(model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "# training parameters\n",
    "learning_rate = 3e-3\n",
    "opt = torch.optim.Adam(model.parameters(), learning_rate)\n",
    "max_iter = 100       \n",
    "n_norm = 7           # n_norm is average number of convolution neighbors per atom\n",
    "batch_size = 500\n",
    "training_size = len(training_dataset)\n",
    "print(training_size)\n",
    "training_dataloader = tg.data.DataListLoader(training_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration     1    batch    10 /    10  loss =   528.007105    elapsed =    3.12 s\n",
      "Iteration    10    batch    10 /    10  loss =    70.093726    elapsed =    3.04 s\n",
      "Iteration    16    batch     6 /    10  loss =    45.119349    elapsed =    3.06 s\r"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "torch.cuda.empty_cache()\n",
    "model.to(device)\n",
    "\n",
    "n_batches = int(training_size / batch_size)\n",
    "for i in range(max_iter):\n",
    "    start_time = time.time()\n",
    "    loss_cum = torch.tensor([0.]).to(device)\n",
    "    for j,data in enumerate(training_dataloader):\n",
    "        print(f\"Iteration {i+1:5d}    batch {j+1:5d} / {n_batches:5d}\", end=\"\\r\", flush=True)\n",
    "\n",
    "        data = tg.data.Batch.from_data_list(data)\n",
    "        data.to(device)\n",
    "        output = model(data.x, data.edge_index, data.edge_attr, n_norm=n_norm)\n",
    "        loss, _ = loss_function(output, data)\n",
    "        loss_cum += loss.detach().pow(2)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    end_time = time.time()\n",
    "    elasped_time = end_time - start_time\n",
    "    loss_cum = (loss_cum/(j+1)).pow(0.5)\n",
    "    print(f\"Iteration {i+1:5d}    batch {j+1:5d} / {n_batches:5d}  loss = {loss_cum.item():12.6f}    elapsed = {elasped_time:7.2f} s\", end=\"\\r\", flush=True)\n",
    "    if i == 0 or (i+1) % 10 == 0:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model as it now\n",
    "torch.cuda.empty_cache()\n",
    "testing_dataloader = tg.data.DataListLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "testing_size = len(test_dataset)\n",
    "n_batches = int(testing_size / batch_size)\n",
    "results_dict = {} # molecule name -> residuals (n_examples,n_atoms)\n",
    "start_time = time.time()\n",
    "\n",
    "loss_cum = torch.tensor([0.]).to(device)\n",
    "for j,data in enumerate(testing_dataloader):\n",
    "    print(f\"batch {j+1:5d} / {n_batches:5d}\", end=\"\\r\", flush=True)\n",
    "    data = tg.data.Batch.from_data_list(data)\n",
    "    data.to(device)\n",
    "    with torch.no_grad():\n",
    "        # run model\n",
    "        output = model(data.x, data.edge_index, data.edge_attr, n_norm=n_norm)\n",
    "        \n",
    "        # compute MSE\n",
    "        loss, residuals = loss_function(output,data)\n",
    "        loss_cum += loss.pow(2)\n",
    "        \n",
    "        # rescale residuals back to ppm and store\n",
    "        residuals = residuals.squeeze(-1).cpu().numpy()\n",
    "        i=0\n",
    "        for name in data.name:\n",
    "            molecule = molecules_dict[name]\n",
    "            n_atoms = molecule.n_atoms\n",
    "            scaling_factors = molecule.scaling_factors\n",
    "            if name not in results_dict:\n",
    "                results_dict[name] = []\n",
    "            subset = residuals[i:i+n_atoms] * scaling_factors\n",
    "            results_dict[name].append(subset)\n",
    "            i += n_atoms\n",
    "            \n",
    "loss_cum = loss_cum/(j+1)\n",
    "loss_cum = loss_cum.pow(0.5)\n",
    "end_time = time.time()\n",
    "elasped_time = end_time - start_time\n",
    "print(f\"\\nOverall loss is {loss_cum.item():.6f}.  Evaluation took {elasped_time:.2f} s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape residual data\n",
    "results_dict2 = {}\n",
    "for name,results in results_dict.items():\n",
    "    results = np.array(results).T\n",
    "    molecule = molecules_dict[name]\n",
    "    atomic_symbols = molecule.atomic_symbols\n",
    "    for i,v in enumerate(results):\n",
    "        element = atomic_symbols[i]\n",
    "        if element not in relevant_elements:\n",
    "            continue\n",
    "        label = f\"{name}_{element}{i+1}\"\n",
    "        results_dict2[label]=v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary stats\n",
    "df = pd.DataFrame(results_dict2)\n",
    "means = df.mean()\n",
    "ranges = df.max()-df.min()\n",
    "RMSEs = np.sqrt(df.pow(2).mean())\n",
    "df = pd.concat([means,ranges,RMSEs], axis=1)\n",
    "df.columns = [\"mean\",\"range\",\"RMSE\"]\n",
    "df = df.round(2)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
