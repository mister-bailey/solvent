# === You can include ordinary training and model sections that apply
# === in general to exploration runs
[training]
resume = True
time_limit = 30:00
example_limit = 100000

[exploration]
run_dir = runs

# status file is relative to run_dir
search_status = search.status

# the number of random, working models to generate before starting exploration
random_samples = 5000
# sample file is relative to run_dir
sample_file = samples.dat
# if sample file doesn't have enough samples, will generate more

# training cycles follow schedule
# after end of schedule, proceed by doubling
example_schedule = [100000]
time_schedue = [1:00:00]

# number of active runs to keep at each step
# after end of schedule, keep constant number of active runs
active_schedule = [32,16,8,4]

# number of new runs to try at each step
# after end of schedule, keep trying a constant number of new runs
try_schedule = [32,16,8,4,2]

# Here's how it works:
# At step N, first we bring our active_schedule[N]
# best runs up to the training limit.
# Then we try try_schedule[N] new runs, bringing
# them up to the limit unless performance
# recommends early stopping.

# Now I have two policy options, and I'm not sure which is best:

# 1. Select the active_schedule[N+1] best runs as our new best
#    as measured by their fitted asymptotes.
# 2. Select our new best only from those that have trained up
#    to the current limit.

#max_epoch = 0
#max_example = 0
#max_time = 0:00


